{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# =========================================================\n",
    "# --- Opcjonalnie : montowanie Google Drive ---\n",
    "# =========================================================\n",
    "# from google.colab import drive\n",
    "# import os\n",
    "#\n",
    "# # Montowanie dysku Google\n",
    "# drive.mount('/content/drive')\n",
    "#\n",
    "# # Ścieżka do folderu roboczego na Drive\n",
    "# base_export_dir = '/content/drive/MyDrive/colab_cifar10_exports'\n",
    "# os.makedirs(base_export_dir, exist_ok=True)\n",
    "#\n",
    "# print(f\"[OK] Google Drive podłączony. Pliki będą zapisywane w:\\n{base_export_dir}\")"
   ],
   "metadata": {
    "id": "kJ6ulNn1VESY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QpeMSHo_A6Aq",
    "ExecuteTime": {
     "end_time": "2025-11-21T11:11:36.418651Z",
     "start_time": "2025-11-21T11:11:36.136247Z"
    }
   },
   "source": [
    "# =========================================================\n",
    "# --- 0. Import bibliotek ---\n",
    "# =========================================================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    BatchNormalization,\n",
    "    Activation,\n",
    "    MaxPooling2D,\n",
    "    GlobalAveragePooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout\n",
    ")\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# =========================================================\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m# --- 0. Import bibliotek ---\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# =========================================================\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmatplotlib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplt\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mdatetime\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'numpy'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================================================\n",
    "# --- 1. Importowanie danych CIFAR10 ---\n",
    "# =========================================================\n",
    "# Wczytanie zbioru danych CIFAR10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Wyświetlenie przykładowego obrazu (opcjonalne)\n",
    "print(\"Etykieta przykładowego obrazu:\", y_train[0])\n",
    "plt.imshow(x_train[0])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "izWbwVvlVYe1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================================================\n",
    "# --- 2. Wstępne przetwarzanie danych ---\n",
    "# =========================================================\n",
    "# Normalizacja do zakresu [0,1] i dodanie kanału (H,W,1)\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test  = x_test.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encoding etykiet\n",
    "num_classes = 10\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_test_cat  = to_categorical(y_test, num_classes)\n",
    "\n",
    "# lista nazw klas CIFAR-10 (użyteczna w wyświetleniach)\n",
    "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
   ],
   "metadata": {
    "id": "GPlEf-TEVeM1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "dodanie sieci BathNormalization dla unikniecia przetrenowania modelu oraz dla stabilizacji gradientu (bo tutaj sa bardzo rozpikselowane obrazy)\n",
    "po tej zmianie var_loss z 0.93 na 0.83\n",
    "\n",
    "oryginalne dane:\n",
    "\n",
    "    model = Sequential([\n",
    "      oryginalna werjsa\n",
    "      Input(shape=input_shape),\n",
    "      Conv2D(32, (3,3), activation='relu'),\n",
    "      MaxPooling2D((2,2)),\n",
    "      Conv2D(64, (3,3), activation='relu'),\n",
    "      MaxPooling2D((2,2)),\n",
    "      Flatten(),\n",
    "      Dense(64, activation='relu'),\n",
    "      Dense(num_classes, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================================================\n",
    "# --- 3. Definicja modelu sieci neuronowej ---\n",
    "# =========================================================\n",
    "input_shape = (32, 32, 3)  # CIFAR-10 to 32x32 RGB\n",
    "\n",
    "#todo co sie po kolei dzieje wiedziec\n",
    "model = Sequential([\n",
    "    Input(shape=input_shape),\n",
    "\n",
    "    #---------\n",
    "    Conv2D(32, (3,3), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Dropout(0.1), #todo co robi dropout opisac i wiedziec\n",
    "\n",
    "    #---------\n",
    "    Conv2D(64, (3,3), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Dropout(0.2),\n",
    "\n",
    "    #---------\n",
    "\n",
    "    #siec gesta\n",
    "    Flatten(),\n",
    "    Dense(128),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])"
   ],
   "metadata": {
    "id": "2J00gbJwVdRF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Zmiana: optimizer = Adam (learing_rate =  1e-2) na learning_rate = 1e-3 -> to zmniejsza niestabilnosc walidacji, program nie \"skacze\" po wynikach tylko przechodzi powoli\n",
    "\n",
    "to zmienia oryginalne 1.35 var_loss na 0.93"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================================================\n",
    "# --- 4. Kompilacja modelu ---\n",
    "# =========================================================\n",
    "# Optymalizator Adam, funkcja straty categorical_crossentropy, metryka accuracy\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Wyświetlenie podsumowania modelu\n",
    "model.summary()"
   ],
   "metadata": {
    "id": "GdfWoNFOVcjV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "zmiana w EarlyStopping -> monitorujemy val_loss wiec na tym trzeba sie skupic, dodatkowo przywrocenie najlepszych wag na true, oraz zmiana patience na 5\n",
    "\n",
    "zmiana w ReduceLROnPlateau -> tak samo, monitorowanie val_loss zamiast val_accuracy, zmiana patience na 3\n",
    "\n",
    "oryginalna wersja (ale po moich zmianach):\n",
    "\n",
    "    es = EarlyStopping(\n",
    "      monitor='val_accuracy',   # metryka, którą obserwujemy (np. val_loss, val_accuracy)\n",
    "      patience=6,               # liczba epok bez poprawy, po których zatrzymujemy trening\n",
    "      min_delta=1e-5,           # minimalna wymagana zmiana, by uznać, że jest „poprawa”\n",
    "      mode='max'              # 'min' jeśli monitorujemy straty, 'max' jeśli dokładność\n",
    "    )\n",
    "    rlp = ReduceLROnPlateau(\n",
    "      monitor='val_accuracy',   # metryka do obserwacji\n",
    "      factor=0.5,           # ile razy zmniejszyć LR (tu: o połowę)\n",
    "      patience=6,           # liczba epok bez poprawy przed zmniejszeniem LR, rlp.patience < es.patience\n",
    "      min_delta=1e-5,       # próg czułości jak wyżej\n",
    "      min_lr=1e-4,          # dolna granica learning rate\n",
    "      mode='max',           # 'min' dla strat, 'max' dla dokładności\n",
    "    )\n",
    "\n",
    "to zmniejsza var_loss z okolo 0.70 na 0.66"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================================================\n",
    "# --- 5. Trenowanie modelu ---\n",
    "# =========================================================\n",
    "es = EarlyStopping(\n",
    "    monitor='val_loss',   # metryka, którą obserwujemy (np. val_loss, val_accuracy)\n",
    "    patience=5,               # liczba epok bez poprawy, po których zatrzymujemy trening\n",
    "    min_delta=1e-5,           # minimalna wymagana zmiana, by uznać, że jest „poprawa”\n",
    "    mode='min',              # 'min' jeśli monitorujemy straty, 'max' jeśli dokładność\n",
    "    restore_best_weights=True\n",
    ")\n",
    "rlp = ReduceLROnPlateau(\n",
    "    monitor='val_loss',   # metryka do obserwacji\n",
    "    factor=0.5,           # ile razy zmniejszyć LR (tu: o połowę)\n",
    "    patience=3,           # liczba epok bez poprawy przed zmniejszeniem LR, rlp.patience < es.patience\n",
    "    min_delta=1e-5,       # próg czułości jak wyżej\n",
    "    min_lr=1e-5,          # dolna granica learning rate\n",
    "    mode='min',           # 'min' dla strat, 'max' dla dokładności\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train_cat,\n",
    "    batch_size=64,\n",
    "    epochs=25,\n",
    "    validation_data=(x_test, y_test_cat),\n",
    "    callbacks=[es, rlp],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Data skończenia treningu - timestamp - znacznik do zapisywania plików\n",
    "ts = datetime.datetime.now().strftime(\"_%Y%m%d_%H%M\")"
   ],
   "metadata": {
    "id": "5iGZqxQjVbyU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================================================\n",
    "# --- 6. Ewaluacja modelu ---\n",
    "# =========================================================\n",
    "loss, accuracy = model.evaluate(x_test, y_test_cat, verbose=0)\n",
    "print('Dokładność na zbiorze testowym:', f\"{accuracy:.2f}\")\n",
    "print('Strata na zbiorze testowym:', f\"{loss:.2f}\")"
   ],
   "metadata": {
    "id": "TKqce0WCVaP9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================================================\n",
    "# --- 7. Wizualizacja przebiegu treningu (loss) ---\n",
    "# =========================================================\n",
    "fig_loss_acc = plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], label='loss (train)')\n",
    "plt.plot(history.history['val_loss'], label='loss (val)')\n",
    "plt.xlabel('Epoka')\n",
    "plt.ylabel('categorical_crossentropy')\n",
    "plt.title(f'Strata na zbiorze testowym to {loss:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['accuracy'], label='accuracy (train)')\n",
    "plt.plot(history.history['val_accuracy'], label='accuracy (val)')\n",
    "plt.xlabel('Epoka')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f\"Dokładność na zbiorze testowym to {accuracy:.2f}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Zapis wykresu\n",
    "loss_acc_path = os.path.join(base_export_dir, f'training_loss_accuracy{ts}.png')\n",
    "fig_loss_acc.savefig(loss_acc_path)\n",
    "print(\"Zapisano wykres loss/accuracy ->\", loss_acc_path)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "rRxS1tpMVaqm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================================================\n",
    "# --- 8. Wizualizacja błędnych klasyfikacji i macierz pomyłek ---\n",
    "# =========================================================\n",
    "# Predykcje (etykiety)\n",
    "pred_probs = model.predict(x_test)\n",
    "pred_labels = np.argmax(pred_probs, axis=1)\n",
    "true_labels = np.argmax(y_test_cat, axis=1)  # lub po prostu y_test\n",
    "\n",
    "# Indeksy błędnych klasyfikacji\n",
    "incorrect_indices = np.nonzero(pred_labels != true_labels)[0]\n",
    "\n",
    "# Wyświetlenie kilku błędnych przykładów\n",
    "n_show = min(3, len(incorrect_indices))\n",
    "for i in range(n_show):\n",
    "    idx = incorrect_indices[i]\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(x_test[idx].squeeze(), cmap='gray')\n",
    "    plt.title(f\"Prawidłowo: {class_names[true_labels[idx]]}  -> Predykcja: {class_names[pred_labels[idx]]}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "fig_cm, ax = plt.subplots(figsize=(8,8))\n",
    "disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
    "plt.title('Macierz pomyłek')\n",
    "# Zapis wykresu\n",
    "loss_acc_path = os.path.join(base_export_dir, f'confusion_matrix{ts}.png')\n",
    "fig_cm.savefig(loss_acc_path)\n",
    "print(\"Zapisano macierz pomyłek ->\", loss_acc_path)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "h-ZEblWvVZqO"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
